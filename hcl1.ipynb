{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hcl1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "cb8jFjwrC1IA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "from tensorflow.keras import layers\n",
        "import os\n",
        "li_text=[]\n",
        "all_words=[]\n",
        "#li_files=os.listdir('D:/dataset/New folder/HCL ML Challenge/HCL ML Challenge Dataset')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M61RNOx7fypf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results0=pd.read_csv('D:/dataset/New folder/HCL ML Challenge/results0.csv')\n",
        "li_files=results0['Filename']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06BgJkIPZXoS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sam1=open('D:/dataset/New folder/HCL ML Challenge/Sample Dataset/sample2_0000001R.txt','r').read()\n",
        "sam0=open('D:/dataset/New folder/HCL ML Challenge/Sample Dataset/sample1_0000001R.txt','r').read()\n",
        "out=pd.read_csv('D:/dataset/New folder/HCL ML Challenge/sample_submission.csv')\n",
        "out_0=out.values[0][1]\n",
        "\n",
        "out_1=out.values[1][1]\n",
        "pun=['{','.','nan',':',',',\"'\",'``', \"''\", '``','}','Â£']\n",
        "\n",
        "\n",
        "def featurise(doc,out):\n",
        "    doc=doc.replace(',','')\n",
        "    pun=['{','.','nan',':',',',\"'\",'``', \"''\", '``','}','Â£']\n",
        "    doc=word_tokenize(doc)\n",
        "    out=word_tokenize(out)\n",
        "    doc_pun=[]\n",
        "    for i in doc:\n",
        "        if i not in pun:\n",
        "            doc_pun.append(i)\n",
        "    out_pun=[]\n",
        "    for i in out:\n",
        "        if i not in pun:\n",
        "            out_pun.append(i)\n",
        "    dat=[]\n",
        "\n",
        "    for i in doc_pun:\n",
        "        if i in out_pun:\n",
        "            dat.append(1)\n",
        "        else:\n",
        "            dat.append(0)\n",
        "    return pd.DataFrame({'word':doc_pun,\"present\":dat})\n",
        "\n",
        "table1=featurise(sam1,out_1)\n",
        "table0=featurise(sam0,out_0)\n",
        "table1.to_csv('D:/dataset/New folder/HCL ML Challenge/table1.csv')\n",
        "table0.to_csv('D:/dataset/New folder/HCL ML Challenge/table0.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Z7GFxBADRK7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in li_files:\n",
        "    f=open('D:/dataset/New folder/HCL ML Challenge/HCL ML Challenge Dataset/{}.txt'.format(i),'r')\n",
        "    \n",
        "    st=f.read()\n",
        "    st=st.replace(',','')\n",
        "    st=word_tokenize(st)\n",
        "    pun=['{','.','nan',':',',',\"'\",'``', \"''\", '``','}','Â£']\n",
        "    st_pun=[]\n",
        "    for i in st:\n",
        "        if i not in pun:\n",
        "            st_pun.append(i)\n",
        "            if (i.isnumeric() != True)|(i=='2019'):\n",
        "                all_words.append(i)\n",
        "                \n",
        "    li_text.append(st_pun)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQiV-k0VDY-N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_words=list(nltk.FreqDist(all_words).keys())\n",
        "\n",
        "table0=pd.read_csv('D:/dataset/New folder/HCL ML Challenge/table0.csv')\n",
        "table1=pd.read_csv('D:/dataset/New folder/HCL ML Challenge/table1.csv')\n",
        "\n",
        "seq_len=10\n",
        "vocab_len=len(all_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCsB5VZlDwUw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def featurise(doc):\n",
        "    feature=[]\n",
        "    for i in doc:\n",
        "        if (i.isnumeric())&(i!='2019'):\n",
        "            feature.append(vocab_len)\n",
        "        else:\n",
        "            feature.append(all_words.index(i))\n",
        "    return feature\n",
        "\n",
        "features=[]\n",
        "for doc in li_text:\n",
        "    feat_text=featurise(doc)\n",
        "    features.append(feat_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntiw-hhzD0Cf",
        "colab_type": "code",
        "outputId": "2d7b8a08-3362-4716-cb43-ea12b34bf1ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "seq_feature=[]\n",
        "for doc in features:\n",
        "    temp=[]\n",
        "    for i in range(0,len(doc)-10+1):\n",
        "        seq=[]\n",
        "        for j in range(0,seq_len):\n",
        "            seq.append(doc[j+i])\n",
        "        temp.append(seq)\n",
        "    seq_feature.append(temp)\n",
        "    \n",
        "seq_lable=[table0['present'][seq_len-1:],table1['present'][seq_len-1:]]\n",
        "len(seq_lable[0])\n",
        "#len(seq_lable[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "276"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Hty6h5TD4zk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model=tf.keras.Sequential([\n",
        "    layers.Embedding(vocab_len+1,256,input_length=10),\n",
        "    layers.LSTM(1024,return_sequences=True),\n",
        "    layers.LSTM(1024),\n",
        "    layers.Dense(2,activation='softmax')\n",
        "    ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRRM_R8aZNx9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "z=list(seq_lable[0])\n",
        "for i in seq_lable[1]:\n",
        "  z.append(i)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dS0t3-ymD793",
        "colab_type": "code",
        "outputId": "4821ccfa-60fb-4304-dcc2-0582de94c5d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "opt = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)\n",
        "model.compile(optimizer=opt,loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
        "model.fit(np.array(seq_feature[0]+seq_feature[1]),np.array(z),epochs=40)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 518 samples\n",
            "Epoch 1/40\n",
            "518/518 [==============================] - 2s 4ms/sample - loss: 0.4412 - accuracy: 0.7992\n",
            "Epoch 2/40\n",
            "518/518 [==============================] - 0s 716us/sample - loss: 0.4104 - accuracy: 0.8822\n",
            "Epoch 3/40\n",
            "518/518 [==============================] - 0s 722us/sample - loss: 0.2867 - accuracy: 0.8764\n",
            "Epoch 4/40\n",
            "518/518 [==============================] - 0s 724us/sample - loss: 0.2609 - accuracy: 0.8919\n",
            "Epoch 5/40\n",
            "518/518 [==============================] - 0s 705us/sample - loss: 0.2190 - accuracy: 0.9015\n",
            "Epoch 6/40\n",
            "518/518 [==============================] - 0s 709us/sample - loss: 0.1782 - accuracy: 0.9112\n",
            "Epoch 7/40\n",
            "518/518 [==============================] - 0s 724us/sample - loss: 0.1560 - accuracy: 0.9305\n",
            "Epoch 8/40\n",
            "518/518 [==============================] - 0s 710us/sample - loss: 0.1224 - accuracy: 0.9440\n",
            "Epoch 9/40\n",
            "518/518 [==============================] - 0s 706us/sample - loss: 0.1069 - accuracy: 0.9595\n",
            "Epoch 10/40\n",
            "518/518 [==============================] - 0s 714us/sample - loss: 0.1127 - accuracy: 0.9537\n",
            "Epoch 11/40\n",
            "518/518 [==============================] - 0s 714us/sample - loss: 0.0847 - accuracy: 0.9672\n",
            "Epoch 12/40\n",
            "518/518 [==============================] - 0s 710us/sample - loss: 0.0910 - accuracy: 0.9614\n",
            "Epoch 13/40\n",
            "518/518 [==============================] - 0s 710us/sample - loss: 0.0745 - accuracy: 0.9730\n",
            "Epoch 14/40\n",
            "518/518 [==============================] - 0s 709us/sample - loss: 0.0831 - accuracy: 0.9633\n",
            "Epoch 15/40\n",
            "518/518 [==============================] - 0s 722us/sample - loss: 0.1029 - accuracy: 0.9614\n",
            "Epoch 16/40\n",
            "518/518 [==============================] - 0s 722us/sample - loss: 0.0532 - accuracy: 0.9749\n",
            "Epoch 17/40\n",
            "518/518 [==============================] - 0s 714us/sample - loss: 0.0661 - accuracy: 0.9749\n",
            "Epoch 18/40\n",
            "518/518 [==============================] - 0s 714us/sample - loss: 0.0447 - accuracy: 0.9807\n",
            "Epoch 19/40\n",
            "518/518 [==============================] - 0s 728us/sample - loss: 0.0592 - accuracy: 0.9788\n",
            "Epoch 20/40\n",
            "518/518 [==============================] - 0s 711us/sample - loss: 0.0335 - accuracy: 0.9884\n",
            "Epoch 21/40\n",
            "518/518 [==============================] - 0s 714us/sample - loss: 0.0191 - accuracy: 0.9942\n",
            "Epoch 22/40\n",
            "518/518 [==============================] - 0s 709us/sample - loss: 0.0284 - accuracy: 0.9884\n",
            "Epoch 23/40\n",
            "518/518 [==============================] - 0s 714us/sample - loss: 0.0291 - accuracy: 0.9903\n",
            "Epoch 24/40\n",
            "518/518 [==============================] - 0s 724us/sample - loss: 0.0091 - accuracy: 1.0000\n",
            "Epoch 25/40\n",
            "518/518 [==============================] - 0s 724us/sample - loss: 0.0100 - accuracy: 0.9942\n",
            "Epoch 26/40\n",
            "518/518 [==============================] - 0s 774us/sample - loss: 0.0178 - accuracy: 0.9942\n",
            "Epoch 27/40\n",
            "518/518 [==============================] - 0s 717us/sample - loss: 0.0230 - accuracy: 0.9942\n",
            "Epoch 28/40\n",
            "518/518 [==============================] - 0s 712us/sample - loss: 0.0675 - accuracy: 0.9826\n",
            "Epoch 29/40\n",
            "518/518 [==============================] - 0s 712us/sample - loss: 0.0878 - accuracy: 0.9826\n",
            "Epoch 30/40\n",
            "518/518 [==============================] - 0s 708us/sample - loss: 0.0804 - accuracy: 0.9865\n",
            "Epoch 31/40\n",
            "518/518 [==============================] - 0s 712us/sample - loss: 0.0995 - accuracy: 0.9710\n",
            "Epoch 32/40\n",
            "518/518 [==============================] - 0s 716us/sample - loss: 0.0660 - accuracy: 0.9768\n",
            "Epoch 33/40\n",
            "518/518 [==============================] - 0s 712us/sample - loss: 0.0209 - accuracy: 0.9961\n",
            "Epoch 34/40\n",
            "518/518 [==============================] - 0s 726us/sample - loss: 0.0203 - accuracy: 0.9961\n",
            "Epoch 35/40\n",
            "518/518 [==============================] - 0s 720us/sample - loss: 0.0090 - accuracy: 1.0000\n",
            "Epoch 36/40\n",
            "518/518 [==============================] - 0s 707us/sample - loss: 0.0044 - accuracy: 0.9981\n",
            "Epoch 37/40\n",
            "518/518 [==============================] - 0s 726us/sample - loss: 0.0024 - accuracy: 1.0000\n",
            "Epoch 38/40\n",
            "518/518 [==============================] - 0s 728us/sample - loss: 0.0010 - accuracy: 1.0000\n",
            "Epoch 39/40\n",
            "518/518 [==============================] - 0s 712us/sample - loss: 5.8358e-04 - accuracy: 1.0000\n",
            "Epoch 40/40\n",
            "518/518 [==============================] - 0s 712us/sample - loss: 4.1318e-04 - accuracy: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x13f0f412508>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_0QULRC4SGT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('model1_hcl.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWWTrkzBRidy",
        "colab_type": "code",
        "outputId": "e6ca3d13-e20d-49c4-e359-cda8f26978cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        "import json\n",
        "\n",
        "\n",
        "def predict_values1(text_file_no):\n",
        "  a=model.predict(seq_feature[text_file_no])\n",
        "  v=0\n",
        "  t=9\n",
        "  output_text=[]\n",
        "  for i in a:\n",
        "    if i[0]<i[1]:\n",
        "      f=seq_feature[text_file_no][t-9][9]\n",
        "      if f!=len(all_words):\n",
        "        output_text.append(all_words[f])\n",
        "      else:\n",
        "        output_text.append(li_text[text_file_no][t])\n",
        "    v+=1\n",
        "    t+=1\n",
        "  #print(output_text)\n",
        "  \n",
        "  temp=''\n",
        "  output='{'\n",
        "  flag=0\n",
        "  for i in output_text:\n",
        "    if i.isnumeric()!=True and i!='(' and i!=')':\n",
        "      temp+=i+' '\n",
        "    elif i=='(':\n",
        "      flag=1\n",
        "    elif i==')':\n",
        "      flag=0\n",
        "    else:\n",
        "      if flag==0:\n",
        "        temp=temp[:len(temp)-1]\n",
        "        output+='\"'+temp+'\"'+':'+'\"'+i+'\"'+','\n",
        "        temp=''\n",
        "      elif flag==1:\n",
        "        temp=temp[:len(temp)-1]\n",
        "        output+='\"'+temp+'\"'+':'+'\"-'+i+'\"'+','\n",
        "        temp=''\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "  output=output[:len(output)-1]\n",
        "  output+='}'\n",
        "  #return output\n",
        "  return json.loads(output)\n",
        "\n",
        "\n",
        "predict_values1(1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Fixed assets Intangible assets': '9423',\n",
              " 'Tangible assets': '6414',\n",
              " 'Current assets Debtors amounts falling due within one year': '113831',\n",
              " 'Cash at bank and in hand': '122352',\n",
              " '': '70587',\n",
              " 'Creditors amounts falling due within one year': '110924',\n",
              " 'Net current assets': '125259',\n",
              " 'Net assets': '141096',\n",
              " 'Capital and reserves Called up share capital': '70587',\n",
              " 'Profit and loss account': '70509',\n",
              " 'Shareholders funds': '141096'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 182
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcZhtUg8Wnbn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "res=[]\n",
        "for i in range(2,502):\n",
        "  res.append(predict_values1(i))\n",
        "  \n",
        "\n",
        "results=pd.DataFrame({'Filename':li_files[2:],'Extracted Values':res})\n",
        "results.to_csv('C:/Users/ninad/Desktop/Results.csv',index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCeGVahJj-Hn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results=pd.DataFrame({'Filename':li_files[2:],'Extracted Values':res})\n",
        "results.to_csv('C:/Users/ninad/Desktop/Results0.csv',index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}